{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43e0fe86",
   "metadata": {},
   "source": [
    "# General introduction to linear classification\n",
    "Problem: given a set of data, assume that they are linearly separable and find a good boundary between the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a0ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab2606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aafcc4f",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "For the examples, we are going to use the [**Recipe Ingredients Dataset**](https://www.kaggle.com/datasets/kaggle/recipe-ingredients-dataset).\n",
    "\n",
    "Each recipe is a collection of ingredients and the task is to guess the kind of cuisine (multi-class classification).\n",
    "\n",
    "First, we compute some statistics on the dataset in order to find the most specific ingredients for each type of cuisine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444c4ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8e8921",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = \"/Users/flint/Data/recipe/kaggle_recipes/train.json\"\n",
    "with open(train_data_file, 'r') as infile:\n",
    "    train_json = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d32948",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisine = defaultdict(list)\n",
    "document_frequency = defaultdict(lambda: 0)\n",
    "cuisine_frequency = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for recipe in train_json:\n",
    "    cuisine[recipe['cuisine']].append(recipe)\n",
    "    for ingredient in recipe['ingredients']:\n",
    "        document_frequency[ingredient] += 1\n",
    "        cuisine_frequency[recipe['cuisine']][ingredient] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e47925",
   "metadata": {},
   "source": [
    "### Specificity of ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628f9cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_ingredients = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "N = sum(document_frequency.values())\n",
    "for c, i in cuisine_frequency.items():\n",
    "    T = sum(i.values())\n",
    "    for x, f in i.items():\n",
    "        kl_ingredients[c][x] = f / T * np.log((f / T) / (document_frequency[x] / N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1846552",
   "metadata": {},
   "outputs": [],
   "source": [
    "KL = pd.DataFrame(kl_ingredients).fillna(-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fc7633",
   "metadata": {},
   "outputs": [],
   "source": [
    "KL.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5b6d5",
   "metadata": {},
   "source": [
    "## Starting with linear regression\n",
    "In order to discuss linear regression, we select a type of cuisine and we create a special dataset where we have only one feature $x$ representing the fraction of that cuisine-specific ingredients in each recipe.\n",
    "\n",
    "The target variable is an arbitrary function $f(x)$ defined over $x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca9e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisine = 'italian'\n",
    "specific_ingredients = set(KL.loc[cuisine][KL.loc[cuisine] > 0].keys())\n",
    "target_function = lambda x: 6 + 2 * x + np.random.randn(X.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27764f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 200\n",
    "X = np.zeros((sample_size, 1))\n",
    "for i, recipe in enumerate(train_json[:sample_size]):\n",
    "    ingredients = recipe['ingredients']\n",
    "    X[i,0] = len([x for x in ingredients if x in specific_ingredients]) / len(ingredients)\n",
    "y = target_function(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d5b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.scatter(X, y, alpha=.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521515c6",
   "metadata": {},
   "source": [
    "### Linear regression model prediction\n",
    "The general form of the linear regression prediction function is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_{n-1} x_{n-1} + \\theta_{n} x_{n}\n",
    "$$\n",
    "\n",
    "where $\\theta_1 \\dots \\theta_n$ are weights for the features and $\\theta_0$ represents a bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acd078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.random.uniform(size=1 + X.shape[1]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc6db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_0 = theta[0,0] + sum([theta[i+1,0]*X[0, i] for i in range(X.shape[1])])\n",
    "y_hat_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e656165",
   "metadata": {},
   "source": [
    "Let's rewrite the function in its vector form\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\theta^T \\cdot \\mathbf{x}\n",
    "$$\n",
    "\n",
    "note that we can manage the free bias term by adding a special feature with value 1 to all the items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c6e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "Xb[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad93ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_0 = theta.T.dot(Xb[0])\n",
    "y_hat_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8b1e65",
   "metadata": {},
   "source": [
    "### Geometric interpretation\n",
    "The model parameters $\\theta$ provide the slope and the intercept for the line that fits the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89860b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_points = np.array([\n",
    "    [1, X.min()], [1, X.max()]\n",
    "])\n",
    "y_prediction = theta.T.dot(example_points.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac31716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.scatter(X, y, alpha=.6)\n",
    "ax.plot(example_points[:,1], y_prediction[0], 'r-')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daa00ff",
   "metadata": {},
   "source": [
    "## Learning the parameters $\\theta$\n",
    "\n",
    "The optimal parameters $\\theta$ that map the features $X$ to the target variable $y$ can be computed by a closed-form solution called $Normal Equation$\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\left ( \\mathbf{X}^T \\cdot \\mathbf{X} \\right )^{-1} \\cdot \\mathbf{X}^T \\cdot y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6769dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_best = np.linalg.inv(Xb.T.dot(Xb)).dot(Xb.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5457e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_points = np.array([\n",
    "    [1, X.min()], [1, X.max()]\n",
    "])\n",
    "y_prediction = theta_best.T.dot(example_points.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.scatter(X, y, alpha=.6)\n",
    "ax.plot(example_points[:,1], y_prediction[0], 'r-')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227cd0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea5932",
   "metadata": {},
   "source": [
    "### Complexity of the Normal Equation as a solution\n",
    "The main limit of the normal equation is that it requires to compute the inverse of $\\mathbf{X}^T \\cdot \\mathbf{X}$ that is $n \\times n$, where $n$ is the number of features. Inverting such matrix as complexity that may reach $O(n^3)$. Thus, this solution is not suitable with a high number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aca3229",
   "metadata": {},
   "source": [
    "## Learning $\\theta$ by Gradient Descent\n",
    "\n",
    "As an alternative to compute the optimal values for $\\theta$, we can try to learn them by finding the values that minimize the error.\n",
    "\n",
    "### Compute prodiction error by a loss function\n",
    "\n",
    "As an example of a loss function, we can use Mean Square Error\n",
    "\n",
    "$$\n",
    "MSE(\\mathbf{X}, \\hat{y}) = \\frac{1}{m} \\sum\\limits_{i=1}^{m} \\left( \\hat{y}^{(i}) - y^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "### Batch gradient descent\n",
    "Then, we need the gradient of the loss function in order to use it for updating the parameters in the direction of the $MSE$ minimum. To compute the gradient vector, we start from the partial derivative of each parameter $\\theta_j$ as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_j} MSE(\\theta) = \\frac{2}{m} \\sum\\limits_{i=1}^{m} \\left ( \\theta^T \\cdot \\mathbf{x}^{(i)} - y^{(i)} \\right ) x_{j}^{(i)}\n",
    "$$\n",
    "\n",
    "That is, in vector form:\n",
    "\n",
    "$$\n",
    "grad_{MSE} = \\frac{2}{m} \\mathbf{X}^T \\cdot (\\mathbf{X} \\cdot \\theta - y)\n",
    "$$\n",
    "\n",
    "Given $grad_{MSE}$ we just need to subtract it to $\\theta$. However, we need to avoid an updating step too big, because we risk to skip the minimum. Thus, let's introduce a learning rate $\\eta$ and do training as:\n",
    "\n",
    "$$\n",
    "\\theta_{i+1} = \\theta_i - \\eta\\ grad_{MSE}(\\theta_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc5cec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = lambda theta: np.power(theta.T.dot(Xb.T)[0] - y, 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e315d3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.5\n",
    "iterations = 1000\n",
    "m = Xb.shape[0]\n",
    "\n",
    "theta = np.random.uniform(size=(Xb.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ada955",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "history = []\n",
    "for iteration in range(iterations):\n",
    "    loss.append(mse(theta))\n",
    "    gradients = 2/m * Xb.T.dot(Xb.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "    if iteration % 20 == 0:\n",
    "        history.append(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d624e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(loss)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9669e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_points = np.array([\n",
    "    [1, X.min()], [1, X.max()]\n",
    "])\n",
    "y_prediction_best = theta_best.T.dot(example_points.T)\n",
    "y_prediction = theta.T.dot(example_points.T)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.scatter(X, y, alpha=.6)\n",
    "ax.plot(example_points[:,1], y_prediction_best[0], 'g-')\n",
    "ax.plot(example_points[:,1], y_prediction[0], 'r--')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893707b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_points = np.array([[1, X.min()], [1, X.max()]])\n",
    "predictions = []\n",
    "for th in history:\n",
    "    predictions.append(th.T.dot(example_points.T))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.scatter(X, y, alpha=.6)\n",
    "for pred in predictions:\n",
    "    ax.plot(example_points[:,1], pred[0], 'r--')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6557dc8",
   "metadata": {},
   "source": [
    "## Exploit Linear regression for binary classification\n",
    "In order to address binary classification, a very simple strategy is to just change the target variable in a categorical variable. In our case, we set $y \\in \\{-1, 1\\}$, with $-1$ if the recipes is not from the selected cuisine and $1$ otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1e7597",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 200\n",
    "X, y = np.zeros((sample_size, 1)), np.ones((sample_size, 1))\n",
    "for i, recipe in enumerate(train_json[:sample_size]):\n",
    "    ingredients = recipe['ingredients']\n",
    "    X[i,0] = len([x for x in ingredients if x in specific_ingredients]) / len(ingredients)\n",
    "    if recipe['cuisine'] != cuisine:\n",
    "        y[i] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ccae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.scatter(X, y, alpha=.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7497aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.5\n",
    "iterations = 1000\n",
    "m = Xb.shape[0]\n",
    "theta = np.random.uniform(size=(Xb.shape[1], 1))\n",
    "loss = []\n",
    "history = []\n",
    "for iteration in range(iterations):\n",
    "    loss.append(mse(theta))\n",
    "    gradients = 2/m * Xb.T.dot(Xb.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "    if iteration % 20 == 0:\n",
    "        history.append(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac823d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_points = np.array([[1, X.min()], [1, X.max()]])\n",
    "y_prediction = theta.T.dot(example_points.T)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.scatter(X, y, alpha=.6)\n",
    "ax.plot(example_points[:,1], y_prediction[0], 'r--')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efd1836",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7330b3fd",
   "metadata": {},
   "source": [
    "### Other algorithm for gradient descent\n",
    "\n",
    "Batch gradient descent is not the only option for training. In order to avoid exploring the whole dataset to estimate the gradients, have a look at two important options:\n",
    "- **Stocastic Gradient Descent**: computes the gradient of the parameters using only a single or a few training items by sampling.\n",
    "- **Mini-batch Gradient Descent**: used often in combination with SGD works of small subsets of the trainin items. Working on minibatches instead of single items reduces the variance in the parameter update and can lead to more stable convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11078d83",
   "metadata": {},
   "source": [
    "## Work with more features\n",
    "\n",
    "Now, instead of trying to predict the class observing the number of cuisine-specific ingredients, we encode the recipe ingredients into one-hot encoding vectors in order to work with more features.\n",
    "\n",
    "Note that the only thing that changes is the shape of the $\\mathbf{X}$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992677da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 200\n",
    "vocabulary = list(pd.Series(document_frequency).sort_values(ascending=False).head(100).keys().values)\n",
    "X, y = np.zeros((sample_size, len(vocabulary))), np.ones((sample_size, 1))\n",
    "for i, recipe in enumerate(train_json[:sample_size]):\n",
    "    ingredients = recipe['ingredients']\n",
    "    for ingredient in ingredients:\n",
    "        try:\n",
    "            X[i, vocabulary.index(ingredient)] = 1\n",
    "        except ValueError:\n",
    "            pass\n",
    "    if recipe['cuisine'] != cuisine:\n",
    "        y[i] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddfdfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "Xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1\n",
    "iterations = 1000\n",
    "m = Xb.shape[0]\n",
    "theta = np.random.uniform(size=(Xb.shape[1], 1))\n",
    "loss = []\n",
    "history = []\n",
    "for iteration in range(iterations):\n",
    "    loss.append(mse(theta))\n",
    "    gradients = 2/m * Xb.T.dot(Xb.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "    if iteration % 20 == 0:\n",
    "        history.append(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5a2553",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 2))\n",
    "ax.plot(loss)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2bc6ff",
   "metadata": {},
   "source": [
    "**note**: we use a transformation function (i.e., sign) to map predictions on binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f453bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = theta.T.dot(Xb.T)[0]\n",
    "np.sign(y_hat[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea71801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d079155",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "Xpca = pca.fit_transform(Xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905e86f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6), ncols=2)\n",
    "ax[0].scatter(Xpca[:,0], Xpca[:,1], alpha=.4, c=y, cmap='coolwarm')\n",
    "ax[1].scatter(Xpca[:,0], Xpca[:,1], alpha=.4, c=np.sign(y_hat), cmap='coolwarm')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855792b",
   "metadata": {},
   "source": [
    "### Evaluation for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b174eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb1b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_size = 100\n",
    "X_test, y_test = np.zeros((test_sample_size, len(vocabulary))), np.ones((test_sample_size, 1))\n",
    "for i, recipe in enumerate(train_json[sample_size:sample_size + test_sample_size]):\n",
    "    ingredients = recipe['ingredients']\n",
    "    for ingredient in ingredients:\n",
    "        try:\n",
    "            X_test[i, vocabulary.index(ingredient)] = 1\n",
    "        except ValueError:\n",
    "            pass\n",
    "    if recipe['cuisine'] != cuisine:\n",
    "        y_test[i] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b510e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb_test = np.c_[np.ones((X_test.shape[0], 1)), X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74a95a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.sign(theta.T.dot(Xb_test.T)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbed607",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mt.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0841b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6), ncols=2)\n",
    "cmd_train = mt.ConfusionMatrixDisplay(confusion_matrix=mt.confusion_matrix(y, np.sign(y_hat)))\n",
    "cmd_test = mt.ConfusionMatrixDisplay(confusion_matrix=mt.confusion_matrix(y_test, y_pred))\n",
    "cmd_train.plot(ax=ax[0], cmap='Greys', colorbar=False)\n",
    "cmd_test.plot(ax=ax[1], cmap='Greys', colorbar=False)\n",
    "ax[0].set_title('Trainset')\n",
    "ax[0].set_title('Testset')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e08a2a",
   "metadata": {},
   "source": [
    "### Learning rates on the training and the test set and the problem of overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b3f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse = lambda theta: np.power(theta.T.dot(Xb_test.T)[0] - y_test, 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ddb4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.2\n",
    "iterations = 5000\n",
    "m = Xb.shape[0]\n",
    "theta = np.random.uniform(size=(Xb.shape[1], 1))\n",
    "loss, accuracy = [], []\n",
    "test_loss, test_accuracy = [], []\n",
    "for iteration in range(iterations):\n",
    "    loss.append(mse(theta))\n",
    "    test_loss.append(test_mse(theta))\n",
    "    accuracy.append(mt.accuracy_score(y, np.sign(theta.T.dot(Xb.T)[0])))\n",
    "    test_accuracy.append(mt.accuracy_score(y_test, np.sign(theta.T.dot(Xb_test.T)[0])))\n",
    "    gradients = 2/m * Xb.T.dot(Xb.dot(theta) - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f631b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4), ncols=2)\n",
    "ax[0].plot(loss, label='Train loss')\n",
    "ax[0].plot(test_loss, label='Test loss')\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].legend()\n",
    "ax[1].plot(accuracy, label='Train accuracy')\n",
    "ax[1].plot(test_accuracy, label='Test accuracy')\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228a1cb",
   "metadata": {},
   "source": [
    "**Idea**. Use the mapping to categorical values in the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821aa91",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "The main idea of logistic regression is to estimate a probability for the target class as follows:\n",
    "\n",
    "$$\n",
    "\\hat{p} = \\sigma(\\theta^T \\cdot \\mathbf{X})\n",
    "$$\n",
    "\n",
    "where $\\sigma(\\cdot)$ is a sigmoid function, such as:\n",
    "\n",
    "$$\n",
    "\\sigma(j) = \\frac{1}{1 + exp(-j)}\n",
    "$$\n",
    "\n",
    "The output $\\hat{p}$ is interpreted as a probability that we can map on $\\hat{y}$ as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\Big\\{  \\begin{array}{ccc}\n",
    "-1 & if & \\hat{p} < 0.5 \\\\\n",
    "1 & if & \\hat{p} \\geq 0.5\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "### Loss function\n",
    "The cost in training for an item can be estimated as (**log loss**):\n",
    "\n",
    "$$\n",
    "\\ell(\\theta) = \\Big\\{  \\begin{array}{ccc}\n",
    "-\\log{\\hat{p}} & if & y = 1 \\\\\n",
    "-\\log{1 - \\hat{p}} & if & y = -1\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "That is general is rewritten with $y \\in {0, 1}$ as\n",
    "\n",
    "$$\n",
    "\\ell(\\theta) = - \\frac{1}{m} \\sum\\limits_{i=1}^{m} \\left[ y^{(i)} \\log \\left( \\hat{p}^{(i)} \\right) + \n",
    "(1 - y^{(i)}) \\log \\left( 1 - \\hat{p}^{(i)} \\right) \\right]\n",
    "$$\n",
    "\n",
    "with partial derivates\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_j} \\ell(\\theta) = \\frac{1}{m} \\sum\\limits_{i=1}^{m} \n",
    "\\left( \\sigma \\left( \\theta^T - \\mathbf{x}^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc8fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf1e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 200\n",
    "X, y = np.zeros((sample_size, 1)), np.ones((sample_size, 1))\n",
    "for i, recipe in enumerate(train_json[:sample_size]):\n",
    "    ingredients = recipe['ingredients']\n",
    "    X[i,0] = len([x for x in ingredients if x in specific_ingredients]) / len(ingredients)\n",
    "    if recipe['cuisine'] != cuisine:\n",
    "        y[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea052159",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression()\n",
    "logistic.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850de8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = np.linspace(0, 1, X.shape[0]).reshape(-1, 1)\n",
    "probs = logistic.predict_proba(fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2062a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.scatter(X, y, alpha=.6)\n",
    "ax.plot(fake_data, probs[:,1], 'g--')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd332a2",
   "metadata": {},
   "source": [
    "## Support Vector Machine (intuition)\n",
    "\n",
    "The basic idea of SVM is to learn a prediction that is featured by the largest margin from the classes.\n",
    "\n",
    "Recall that $\\theta^T \\mathbf{x} = 0$ (including the bias term) is the equation of the hiperplane that separates the data and that we can use the hyperplane for classification by computing\n",
    "\n",
    "$$\n",
    "\\hat{y} = sign(\\theta^T \\mathbf{x})\n",
    "$$\n",
    "\n",
    "However, we can have several different hyperplanes which serve to the same goal by $\\lambda \\theta$ for $\\lambda \\in \\mathbb{R}^{+}$. We can thus define a *canonical hyperplane* as the one that separates the point with a *functional distance* of at least 1.\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_i \\cdot \\theta \\geq 1\\ \\textrm{if}\\ y_i = 1 \\newline\n",
    "\\mathbf{x}_i \\cdot \\theta \\leq -1\\ \\textrm{if}\\ y_i = -1 \\newline\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "y_i(\\mathbf{x}_i \\cdot \\theta) \\geq 1\n",
    "$$\n",
    "\n",
    "Note that the notion of functional distance is referred to the resul value of the classification function, not to the geometric margin. The geomeric distance of the margin is given by:\n",
    "\n",
    "$$\n",
    "\\frac{y_i(\\mathbf{x}_i \\cdot \\theta)}{\\mid\\mid \\theta \\mid\\mid} \\geq \\frac{1}{\\mid\\mid \\theta \\mid\\mid}\n",
    "$$\n",
    "\n",
    "The goal of finding an optimal margin is thus to find a boundary that maximizes the geometric distance to the closest data points. This can be obtained by minimizing $\\mid\\mid \\theta \\mid\\mid$.\n",
    "\n",
    "In other terms, we want to minimize $\\mid\\mid \\theta \\mid\\mid$ without violating the margin constraints $y_i(\\mathbf{x}_i \\cdot \\theta) \\geq 1$. Moreover, we express the minimization problem as follows:\n",
    "\n",
    "$$\n",
    "\\textrm{minimize} \\ \\frac{1}{2} \\theta^T \\cdot \\theta \\newline\n",
    "\\textrm{subject to} \\ y_i(\\mathbf{x}_i \\cdot \\theta) \\geq 1\n",
    "$$\n",
    "\n",
    "Note that we use $\\frac{1}{2} \\theta^T \\cdot \\theta$ instead of $\\mid\\mid \\theta \\mid\\mid$ because it has a simple derivative.\n",
    "\n",
    "### Online SVM\n",
    "We can address the problem by gradient descent using the following loss function.\n",
    "\n",
    "$$\n",
    "\\ell(\\theta) = \\frac{1}{2} \\theta^T \\cdot \\theta + C \\sum\\limits_{i=1}^{m} \\max\\left( 0, 1 - y^{(i)} \\left( \\theta^T \\mathbf{x}^{(i)} \\right) \\right)\n",
    "$$\n",
    "where $C$ is a hyperparameter for balancing. The first sum push the model to find a small $\\theta$ to have a larger margin. The second sum compute the total of all margin violation. Minimizing this function means to find a large margin by minimizing the number of violations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8748e1ae",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "We have seen how we can generalize the idea of linear classification by the following equation:\n",
    "\n",
    "$$\n",
    "\\hat{y} = g(\\theta^T \\cdot \\mathbf{X})\n",
    "$$\n",
    "\n",
    "where $g(\\cdot)$ is a function that maps the predicted value to the target variable. This idea can be graphically represented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b2b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='../imgs/perceptron.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415db324",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../imgs/perceptron-example.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b31e742",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "Perceptron can be easily generalized to handle multi-class problems as the in following image. Note that the only main change is in the dimensionality of $\\theta$. Given $k$ classes, we will have $\\theta \\in \\mathbb{R}^{n \\times k}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4adcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,0,0,1,0,1])\n",
    "th = np.array([.4, .2, 0, .6, .1, .8])\n",
    "a.dot(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f6a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,0,0,1,0,1])\n",
    "th = np.array([[.4, .2, 0, .6, .1, .8], [.4, .2, 0, .6, .1, .0]])\n",
    "a.dot(th.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77bcbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../imgs/ffn.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc22cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / (1 + np.exp(-np.array([1.8, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../imgs/ffn-example.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c502d0",
   "metadata": {},
   "source": [
    "### Main activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c95fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {\n",
    "    'identity': lambda x: x,\n",
    "    'sign': lambda x: np.sign(x),\n",
    "    'sigmoid': lambda x: 1 / (1 + np.exp(-x)),\n",
    "    'ReLU': lambda x: np.array([max([k, 0]) for k in x]),\n",
    "    'tahn': lambda x: (np.exp(2*x) - 1) / (np.exp(2*x) + 1),\n",
    "    'hard tahn': lambda x: np.array([max([min([k, 1]), -1]) for k in x])\n",
    "}\n",
    "data = np.linspace(-5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab948a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(2*len(activation), 2), ncols=len(activation))\n",
    "for i, (fun, f) in enumerate(activation.items()):\n",
    "    ax[i].plot(f(data), 'g--')\n",
    "    ax[i].set_title(fun)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ed03a",
   "metadata": {},
   "source": [
    "### Simple feedforward network using PyTorch\n",
    "\n",
    "Predict the cuisine given the combination of ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83e6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caafaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_json[:15000]\n",
    "test_data = train_json[5000:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf2804",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(pd.Series(document_frequency).sort_values(\n",
    "    ascending=False).head(1000).keys().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bb68e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_labels = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data, vocabulary):\n",
    "    features, labels = [], []\n",
    "    for recipe in data:\n",
    "        features.append([f for f in recipe['ingredients'] if f in vocabulary])\n",
    "        labels.append([recipe['cuisine']])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e319dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs, train_labels = get_data(train_data, vocabulary)\n",
    "test_docs, test_labels = get_data(test_data, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4027cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9a9bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf64791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((len(train_docs), len(vocabulary)))\n",
    "for i, doc in enumerate(train_docs):\n",
    "    for ingredient in doc:\n",
    "        X_train[i, vocabulary.index(ingredient)] = 1\n",
    "y_train = enc_labels.fit_transform(np.array(train_labels)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18997bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.zeros((len(test_docs), len(vocabulary)))\n",
    "for i, doc in enumerate(test_docs):\n",
    "    for ingredient in doc:\n",
    "        X_test[i, vocabulary.index(ingredient)] = 1\n",
    "y_test = enc_labels.transform(np.array(test_labels)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0510d775",
   "metadata": {},
   "source": [
    "### Torch data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efd83f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe48e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x, y) in train_dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c90ee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12938068",
   "metadata": {},
   "source": [
    "### The simple Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be56bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d292ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.softmax(self.fc(x), dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec659fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimpleNet(X_train.shape[1], y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e6ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x, y) in train_dataloader:\n",
    "    p = net(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed989fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b53a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83da215b",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6fbc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1457d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [(doc, lab) for doc, lab in train_dataloader]\n",
    "read_labels = [x.replace('x0_', '') for x in enc_labels.get_feature_names_out()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb8de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(document_tensor, label_tensor):\n",
    "    net.zero_grad()\n",
    "    output = net(document_tensor)\n",
    "    loss = criterion(output, label_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    for p in net.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949097d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 20000\n",
    "print_every = 2000\n",
    "plot_every = 200\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "learning_rate = 0.1\n",
    "\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "epochs = list(range(1, n_iters + 1))\n",
    "for it in tqdm(epochs):\n",
    "    document_tensor, label_tensor = batches[np.random.randint(0, len(batches) - 1)]\n",
    "    output, loss = train(document_tensor, label_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    if it % print_every == 0:\n",
    "        preds = torch.argmax(output, dim=1)\n",
    "        ground = torch.argmax(label_tensor, dim=1)\n",
    "        for i, p in enumerate(preds[:3]):\n",
    "            g = ground[i]\n",
    "            print('{} ==> {}'.format(read_labels[p], read_labels[g]))\n",
    "\n",
    "    if it % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d25dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(all_losses, 'g')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7793ff36",
   "metadata": {},
   "source": [
    "### Prediction on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2fbb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train, y_true_train = [], []\n",
    "for doc, lab in train_dataloader:\n",
    "    output = net(doc)\n",
    "    preds = torch.argmax(output, dim=1)\n",
    "    ground = torch.argmax(lab, dim=1)\n",
    "    for i, p in enumerate(preds):\n",
    "        y_pred_train.append(p)\n",
    "        g = ground[i]\n",
    "        y_true_train.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ba15a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mt.classification_report(y_true_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1746fe20",
   "metadata": {},
   "source": [
    "### Prediction on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3571f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true = [], []\n",
    "for doc, lab in test_dataloader:\n",
    "    output = net(doc)\n",
    "    preds = torch.argmax(output, dim=1)\n",
    "    ground = torch.argmax(lab, dim=1)\n",
    "    for i, p in enumerate(preds):\n",
    "        y_pred.append(p)\n",
    "        g = ground[i]\n",
    "        y_true.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e48c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mt.classification_report(y_true, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e21f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6), ncols=2)\n",
    "cmd_train = mt.ConfusionMatrixDisplay(confusion_matrix=mt.confusion_matrix(y_true_train, y_pred_train), \n",
    "                                      display_labels=read_labels)\n",
    "cmd_test = mt.ConfusionMatrixDisplay(confusion_matrix=mt.confusion_matrix(y_true, y_pred), \n",
    "                                     display_labels=read_labels)\n",
    "cmd_train.plot(ax=ax[0], cmap='Greys', colorbar=False, xticks_rotation='vertical')\n",
    "cmd_test.plot(ax=ax[1], cmap='Greys', colorbar=False, xticks_rotation='vertical')\n",
    "ax[0].set_title('Trainset')\n",
    "ax[0].set_title('Testset')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03052352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
