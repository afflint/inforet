{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "641c8e23",
   "metadata": {},
   "source": [
    "# Example of different information needs\n",
    "For the subset of recipes used in this example, see the [Recipe1M+](http://pic2recipe.csail.mit.edu/) dataset.\n",
    "> Marin, J., Biswas, A., Ofli, F., Hynes, N., Salvador, A., Aytar, Y., ... & Torralba, A. (2019). Recipe1m+: A dataset for learning cross-modal embeddings for cooking recipes and food images. IEEE transactions on pattern analysis and machine intelligence, 43(1), 187-203.\n",
    "\n",
    "See the extracted dataset sample [here](https://unimi2013.sharepoint.com/:u:/s/InformationRetrieval/EaL7kid2qzdCmAA8RO-m5iQBsvCl5cuNIdn0rsJN1FUhSg?e=fdXkkB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0dbe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a854e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3aca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"/Users/flint/Data/recipe/text-sample/\"\n",
    "files = [f for f in os.listdir(folder) if f.endswith('.txt')]\n",
    "recipes = []\n",
    "for file in files:\n",
    "    with open(os.path.join(folder, file), 'r') as data:\n",
    "        recipes.append(data.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8416fb3c",
   "metadata": {},
   "source": [
    "## Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06f410a",
   "metadata": {},
   "source": [
    "### NLTK tokenizer\n",
    "[nltk.tokenize package](https://www.nltk.org/api/nltk.tokenize.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba88b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tokenize = lambda text: [x.lower() for x in nltk.word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a77fe5b",
   "metadata": {},
   "source": [
    "### SpaCy tokenizer and parser\n",
    "See an overview of [SpaCy Linguistic Feature](https://spacy.io/usage/linguistic-features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b4a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1db597",
   "metadata": {},
   "source": [
    "#### Example\n",
    "Recipes do not have a real sentence structure. Thus, we use a special tokenizer for sentences based on newlines. The last chunk is typically the set of instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_sentences = [sentence for sentence in nlp(recipes[0]).sents]\n",
    "newline_sentences = [r.strip('\\n') for r in recipes[0].split('\\n\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc5cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_text = newline_sentences[-1]\n",
    "print(instructions_text)\n",
    "instructions_sentences = list(nlp(instructions_text).sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8992dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = instructions_sentences[0]\n",
    "type(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcedfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for token in sentence:\n",
    "    tokens.append({\n",
    "        'position': token.idx, 'text': token.text, 'pos': token.pos_, 'lemma': token.lemma_,\n",
    "        'alpha': token.is_alpha, 'stop': token.is_stop, 'dep': token.dep_, 'morph': token.morph\n",
    "    })\n",
    "S = pd.DataFrame(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008e328",
   "metadata": {},
   "outputs": [],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824fc4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.displacy import render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "render(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc4b2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
